## Introduction
There are very few paper that have moved me as much as [AlphaEvolve](). The paper improved on the RL pilled frontier of the current AI research landscape, and proved - by discovering novel algorithms, just how much we can still squeeze performance out of LLMs by improving solutions to hard problems and evolving them to discover new solutions. Here's what @karpathy has to say about it, and is more or less what AlphaEvolve does:

>Scaling up RL is all the rage right now. I'm fairly certain RL will continue to yield more intermediate gains, but I also don't expect it to be the full story.
>...
>There's significantly more bits of supervision we extract per rollout via a review/reflect stage of human mechanism of improvement along the lines of "what went well? what didn't go so well? what should I try next time?" etc. and the lessons from this stage feel explicit, like a new string to be added to the system prompt for the future, optionally to be distilled into weights (/intuition) later a bit like sleep.
>...
>Example algorithm: given a task, do a few rollouts, stuff them all into one context window (along with the reward in each case), use a meta-prompt to review/reflect on what went well or not to obtain string "lesson", to be added to system prompt (or more generally modify the current lessons database). Many blanks to fill in, many tweaks possible, not obvious.


When I was going through the paper, something struck to me. The paradigms introduced in AlphaEvolve can be used make progress for *optimization problems*, where there is no exact "algorithmic breakthrough", but rather heuristic or cleverness breakthrough which is something that can't be learned through pattern recognition prevalent in LLMs.

NP Hard problems by definition have no polynomial time solutions. These are practically unsolvable for larger instances, and often require clever approaches than brute force solutions. Since exact algorithms are computationally prohibitive, the two main strategies used to solve NP Hard problems are *approximation* and *heuristics*. These are not technically easier for LLMs(or any particular system for solving such problems) to excel on these two fronts, as they require extensive information about problem domain and often require manual tuning. 

"Cadence" is an attempt to understand evolutionary problem solving by studying how solutions evolve over generations of improvement, and whether or not it could lead to newer, improved solutions for NP Hard problems. The solutions generated by LLMs directly feeds into the idea of Asymmetric Verifiability, as for TSP it is much easier to verify than to solve. Asymmetric verification and programming problems go hand in hand, as it’s tedious to read code and check its correctness, but if you have test cases with ample coverage, you can quickly check any given solution.

For the purpose of this blog, I chose the *Travelling Salesman Problem*(TSP), and analyzed how the solutions evolves. TSP is a classical optimization problem, with approximate solutions traditionally requiring exponential time to solve(but less time to verify!). [According to Jason Wei](https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law), all tasks that follow the properties defined below, will be solved by AI(*Verifier's Law*):

1. Objective truth: everyone agrees what good solutions are.
2. Fast to verify: any given solution can be verified in a few seconds. 
3. Scalable to verify: many solutions can be verified simultaneously.
4. Low noise: verification is as tightly correlated to the solution quality as possible. 
5. Continuous reward: it’s easy to rank the goodness of many solutions for a single problem.

As far as I my hypothesis go, the stochastic nature of LLMs and their pattern finding ability, this is a really hard problem and they can possibly fail in designing the approximation and heuristics strategies.  While LLMs have demonstrated ability to discover [new mathematical constructions]() for [Cap Set problem](), these have been a result of huge experiments, often requiring resources far beyond the budget of an individual. So, my second hypothesis that I want to prove is whether I can extrapolate the results of huge experiments from a sequence of far smaller, constrained experiments - something that was explored in [Scaling Scaling Laws with Board Games](). Therefore, the aim of this blog is to prove/disprove the following hypothesis:

*Hypothesis 1:* How good are LLMs in designing approximation and heuristics designing for the Travelling Salesman Problem?

*Hypothesis 2:* Can we draw conclusions about scaling TSP problem from this approach with a series of constrained experiments? 

Before moving forward with what I found, let us look into the architectural design of Cadence.

## Architectural Overview - Cadence
At its core, Cadence operates on a continuous feedback loop, where programs are treated as evolving entities. The system maintains a population of programs, each representing a potential solution to the target computational problem. The evolutionary process is driven by the performance of these programs on a fixed test suite, with better-performing programs being favored for reproduction and further refinement. The unique aspect of Cadence lies in its use of LLMs as the primary mechanism for generating new program variations and mutations, guided by explicit instructions and the context of previous program generations.

The operational cycle of Cadence is structured around an iterative evolutionary loop,
which can be summarized as follows:

1. *Parent and Children Sampling:* A parent program is selected from the current population, along with its previously generated children. This provides the LLM with a historical context of the program's lineage and evolutionary trajectory. The parents and the child are stored in a SQLite database, and are sampled from there.
2. *Prompt Construction:* A comprehensive prompt is dynamically constructed. This prompt encapsulates the parent program's code, relevant sections from its children, set of instructions designed to guide the LLM in generating modifications, and feedback every set number of generations. The instructions are critical for directing the LLM towards specific improvements or exploratory behaviors.
3. *LLM-driven Code Generation:* The constructed prompt is fed to an LLM. The LLM then generates modified versions of designated code blocks within the parent program. This step leverages the LLM's understanding of programming logic and its ability to produce syntactically correct and semantically meaningful code variations. In this blog, we've used `gemini-2.5-pro` for code generation.
4. *Diff Application and Child Program Production:* The modifications generated by the LLM, often in the form of code differences (diffs), are applied to the parent program. This process yields a new 'child' program, which inherits characteristics from its parent but incorporates the LLM-suggested changes. For now, this works on functions rather than entire files(similar to what was done in [FunSearch](https://arxiv.org/pdf/2411.19744)). 
5. *Performance Evaluation:* Each newly generated child program undergoes rigorous evaluation against a predefined, fixed test suite. For the Traveling Salesman Problem, this involves assessing the quality (e.g., shortest path length) of the solution produced by the program. The evaluation employs multi-seed deterministic metrics to ensure consistency and reliability of performance measurements. This feeds into the idea of asymmetric verifiability.
6. *Program Promotion:* Periodically, programs that demonstrate superior performance are promoted within the population. These 'elite' programs are given higher priority for selection as parents in subsequent generations, thereby guiding the evolutionary process towards more optimal solutions. These can be set using ELITISM_INTERVAL from the config file.
7. *Meta-prompting (Optional Instruction Mutation):* To foster diversity and prevent premature convergence, Cadence incorporates an optional metaprompting mechanism. This involves periodically mutating the instructions provided to the LLM. This self-adaptive instruction evolution allows Cadence to explore different problem-solving strategies and potentially discover novel approaches that might not be evident with static instructions. We extract lessons from after every set number of instructions, and then give it as a feedback to the LLM. 

## Experimental and Results for Hypothesis 1
(working on it)

The **Travelling Salesman Problem (TSP)** is a classic combinatorial optimization problem: given a list of cities and their pairwise distances, what is the shortest tour that visits each city exactly once and returns to the starting point?

While TSP is known to be computationally hard, **approximate solutions** and **heuristic methods** are widely used in real-world settings. Our central question is:

> _Can Large Language Models (LLMs), guided through structured prompting and iterative feedback, learn to generate effective heuristic solutions to the TSP problem?_

To explore this, we evolve TSP programs using an LLM-based system, letting it **observe**, **modify**, and **evaluate** code over multiple generations.

---
### **Experiment**

#### **Setup**

We define a minimal evolutionary system with the following loop:

1. **Baseline Program**: A trivial solution (e.g., visiting cities in order) is used as a starting point.
2. **Prompt Generation**: The LLM is given the current program and asked to improve it.
3. **Code Generation**: The LLM returns code blocks representing a new strategy.
4. **Evaluation**: Each program is scored based on the total cost (distance) over multiple random city configurations.
5. **Feedback**: After a set number of generation, LLMs extract the lesson from the strategy so far and feed it back to the system prompt as feedback. 

This process is repeated over `N=30` generations. Lessons are distilled every few generations using a meta-prompt that summarizes what strategies worked well (or didn’t).

#### **Code**
Here is an excerpt from the generation loop:

```python
# Run a single generation
result = run_generation(
    generation,
    parent_program,
    inspirations,
    LESSON_INTERVAL=cfg.LESSON_INTERVAL,
)
```

And the evaluation phase:

```python
child_result = execute(child_program_code, task)
add(program_code=child_program_code, metric=child_result["cost"])
```

If `generation % LESSON_INTERVAL == 0`, we extract insights:

```python
lesson = get_lesson_from_history(EXPERIMENT_LOG, previous_lesson)
```

#### **Metrics**

- **Cost**: Total distance of tour across seeds.
- **Feasibility**: Whether all cities are visited once and only once.
- **Improvement**: Drop in cost over generations.
- **Novelty (qualitative)**: Changes in structure of code and algorithms used.

#### **Sample Prompts**

Initial prompt (simplified): This is the prompt we started with.

```
Improve the following TSP solution written in Python.
Do not remove any cities, and make sure to return a valid tour.
```

After a set number of intervals, here's what the feedback looked like after some generations:

```
Farthest insertion and limited 2-opt improved feasibility but not cost effectively; next explore metaheuristics like simulated annealing or genetic algorithms with the nearest neighbor or farthest insertion solution as a starting point, and evaluate the impact of different cooling schedules or population sizes on solution quality and computational cost.
```

```
Ant colony optimization and simulated annealing, despite being global search methods, still fail to reliably produce feasible solutions; shift focus towards hybrid approaches that directly integrate constraint satisfaction techniques (e.g., constraint programming solvers) to guarantee feasibility during the search process, perhaps within a genetic algorithm framework where infeasible solutions are penalized. 
```
---

### **Results**
Here are the results from the experiment(Experiment for 30 generations, with feedback every 5 generations):

<img width="512" height="512" alt="Pasted image 20250718180635" src="https://github.com/user-attachments/assets/6db468ab-e74f-4bad-a1c9-58f75f42c823" />


(there are issues with the change in cost vs generation graph, as it takes into account)
- **Cost vs Generation**: Sharp initial peak, then goes below baseline, and then stays there.
- **Lesson Prompt Influence**: Generations after meta prompt edits showed improvement after some generations.

_Example comparison_:

**Generation 0:**

```python
def tsp(cities): return list(range(len(cities)))
```

And finally, after 30 generations, the solution looks something like this: 
**Generation 30:**

```python

import numpy as np 
import random 
def tsp(cities): 
	num_cities = len(cities) 
	cities_np = np.array(cities) 
	
	def calculate_distance(city1, city2): 
		return np.sum((cities_np[city1] - cities_np[city2])**2)**0.5 
	
	def calculate_tour_distance(tour): 
		distance = 0 
		for i in range(len(tour) - 1): 
			distance += calculate_distance(tour[i], tour[i+1]) 
			distance += calculate_distance(tour[-1], tour[0]) 
		return distance 
	
	def nearest_neighbor_heuristic(): 
		start_city = 0 
		tour = [start_city] 
		unvisited_cities = set(range(num_cities)) 
		unvisited_cities.remove(start_city) 
		current_city = start_city 
		while unvisited_cities: 
			nearest_city = None 
			min_distance = float('inf') 
			for city in unvisited_cities: 
				distance = calculate_distance(current_city, city) 
				if distance < min_distance: 
					min_distance = distance 
					nearest_city = city 
			tour.append(nearest_city) 
			unvisited_cities.remove(nearest_city) 
			current_city = nearest_city 
		return tour 
	tour = nearest_neighbor_heuristic() 
	return tour
```

---
### **Conclusion**

From this small experiment, we can conclude that LLMs exhibit the ability to **learn heuristics** through structured prompting and iterative feedback through lesson . Even with minimal explicit instruction, the model developed:

- Greedy strategies (nearest neighbor)
- Loop structures for exploration
- Valid city permutations

However, performance plateaus without stronger meta-reasoning or curriculum. This validates our hypothesis — **LLMs can design heuristics**, but effectiveness depends on how well the prompt structure and feedback mechanism are crafted.

---

## Experimental and Results for Hypothesis 2

The **Travelling Salesman Problem (TSP)** becomes increasingly difficult as the number of cities increases. Classic heuristics like **Nearest Neighbor (NN)** scale poorly, often producing suboptimal solutions for large instances.

This experiment asks:

> _Can a code-evolving LLM-based system generalize to larger TSP instances when trained only on smaller ones?_

This aligns with recent research in the **"Scaling Laws"** literature - particularly in board games and program synthesis - where systems trained under resource constraints exhibit generalization when scaled up, or break down in predictable ways. So, the goal of this experiment is:

> To compare how the **LLM-evolved TSP heuristics** scale in performance across different input sizes, relative to a classic heuristic (Nearest Neighbor), and to extract any **scaling trends**.

#### **Design Overview**

- **City Sizes**: We sweep over increasing problem sizes (e.g. 6, 8, 10, 12, 14, 16, 18, 20).
- **Baseline**: For each size, we run the **Nearest Neighbor** algorithm multiple times(for 5 generations, and 1 feedback lesson) to compute an average cost and standard deviation.
- **LLM Evolution**: For the same size, we run a full **multi-generation evolutionary loop**, where the LLM is prompted to improve on prior programs.

Each generation in the LLM loop includes:

1. Sampling a parent program
2. Prompting the LLM to improve it (optionally with "lessons")
3. Applying code diffs and evaluating performance
4. Storing metrics and hashing the program
5. Extracting lessons every few generations to steer the LLM more effectively

---

#### **Key Code Snippets**

**1. Baseline Cost Computation**

```python
def baseline_cost(size, seeds=3):
    scores = []
    for seed in range(seeds):
        cities = generate_test_instance(n=size, seed=seed)
        tour = nearest_neighbor(cities)
        scores.append(compute_total_distance(tour, cities))
    return np.mean(scores), np.std(scores)
```

**2. LLM Evolution for a Given Size**

```python
def run_size_experiment(size, generations, lesson_interval):
    task = TSPTask(n_cities=size)
    ...
    for gen in trange(1, generations):
        parent, inspirations = sample(...)
        prompt = build(parent, inspirations, lesson)
        diffs = generate(prompt)
        child_code = apply_diff(parent[3], diffs)
        metric = execute(child_code, task)
        ...
```

**3. Plotting Results**

```python
ax1.plot(cfg.SIZES, final_costs["llm"], marker="s", linestyle="--", label="LLM Evolution")
ax2.plot(cfg.SIZES, rels, marker="o", color="purple")
```

---
### **Conclusion**

**(Graphs will go here after experiment is run)**

- **Graph 1**: Average Tour Cost vs City Count
- **Graph 2**: LLM Relative Improvement over NN (%)

#### **Key Observations (To Be Filled Post-Run)**

| City Count | NN Avg Cost | LLM Cost | Relative Improvement |
| ---------- | ----------- | -------- | -------------------- |
| 6          | xx          | yy       | zz%                  |
| 10         | xx          | yy       | zz%                  |
| ...        | ...         | ...      | ...                  |

### **Insights**

- If relative improvement **increases** with city count, this suggests **emergent scalability** in the LLM's learned heuristic.
- If it **flattens or drops**, then we may need curriculum learning or fine-tuning on larger instances.
- Periodic **lesson extraction** helps the system retain and reuse successful strategies.

---

## **Future Work**

- Try **zero-shot generalization**: train on small sizes, test on large ones.
- Add **runtime** and **token usage** tracking for efficiency metrics.
- Explore **modular policies** that combine small learned components into larger TSP solutions.
